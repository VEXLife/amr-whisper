{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vexs1\\Documents\\datacastle\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import WhisperForConditionalGeneration, WhisperConfig\n",
    "import torch\n",
    "\n",
    "# Create a vocab json including modulation types and 0~31 modulation indices\n",
    "vocab = {\n",
    "    \"<|eos|>\": 0,\n",
    "    \"<|startoftranscript|>\": 1,\n",
    "    \"<|unk|>\": 2,\n",
    "    \"<|pad|>\": 3,\n",
    "    \"<|cls|>\": 4,\n",
    "}\n",
    "vocab_len = len(vocab)\n",
    "added_tokens = [\"<|BPSK|>\", \"<|QPSK|>\", \"<|8PSK|>\", \"<|MSK|>\", \"<|8QAM|>\", \"<|16QAM|>\", \"<|32QAM|>\", \"<|8APSK|>\", \"<|16APSK|>\", \"<|32APSK|>\", \"<|unknownmod|>\"]\n",
    "for symb_wid in torch.linspace(0,1,21):\n",
    "    added_tokens.append(f\"<|{symb_wid:.2f}|>\")\n",
    "for added_token in added_tokens:\n",
    "    vocab[added_token] = vocab_len\n",
    "    vocab_len += 1\n",
    "vocab_len = len(vocab)\n",
    "for i in range(32):\n",
    "    ch = chr(i + ord('0'))\n",
    "    vocab[ch] = vocab_len\n",
    "    vocab_len += 1\n",
    "\n",
    "# Write to vocab.json\n",
    "import json\n",
    "with open(\"vocab.json\", \"w\") as f:\n",
    "    json.dump(vocab, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = WhisperConfig(\n",
    "    vocab_size=vocab_len,\n",
    "    num_mel_bins=2,\n",
    "    max_source_positions=1024,\n",
    "    pad_token_id=vocab[\"<|pad|>\"],\n",
    "    bos_token_id=vocab[\"<|startoftranscript|>\"],\n",
    "    eos_token_id=vocab[\"<|eos|>\"],\n",
    "    decoder_start_token_id=vocab[\"<|startoftranscript|>\"],\n",
    ")\n",
    "model = WhisperForConditionalGeneration(config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence as rnn_utils\n",
    "from einops import rearrange\n",
    "import numpy as np\n",
    "from typing import Tuple\n",
    "\n",
    "symb_type_dict = {\n",
    "    1: \"<|BPSK|>\",\n",
    "    2: \"<|QPSK|>\",\n",
    "    3: \"<|8PSK|>\",\n",
    "    4: \"<|MSK|>\",\n",
    "    5: \"<|8QAM|>\",\n",
    "    6: \"<|16QAM|>\",\n",
    "    7: \"<|32QAM|>\",\n",
    "    8: \"<|8APSK|>\",\n",
    "    9: \"<|16APSK|>\",\n",
    "    10: \"<|32APSK|>\",\n",
    "    11: \"<|unknownmod|>\"\n",
    "}\n",
    "vocab_inv = {v: k for k, v in vocab.items()}\n",
    "symb_type_dict_inv = {v: k for k, v in symb_type_dict.items()}\n",
    "\n",
    "class SignalTokenizer:\n",
    "    def __init__(self, vocab):\n",
    "        self.vocab = vocab\n",
    "        self.__call__ = self.encode\n",
    "\n",
    "    def encode(self, symb_type: int, symb_wid: float, symb_seq: np.ndarray) -> torch.LongTensor:\n",
    "        input_ids = [vocab[symb_type_dict[symb_type]]] + [vocab[f\"<|{symb_wid:.2f}|>\"]] + [vocab['0'] + symb for symb in symb_seq] + [vocab[\"<|eos|>\"]]\n",
    "        return torch.tensor(input_ids, dtype=torch.long)\n",
    "    \n",
    "    def decode(self, input_ids: torch.LongTensor) -> Tuple[int, float, list]:\n",
    "        input_ids = list(input_ids[input_ids != -100])\n",
    "        print(''.join([vocab_inv[input_ids[j].item()] for j in range(len(input_ids))]))\n",
    "        if vocab_inv[input_ids[0].item()] == \"<|startoftranscript|>\":\n",
    "            input_ids = input_ids[1:]\n",
    "        symb_type = symb_type_dict_inv[vocab_inv[input_ids[0].item()]]\n",
    "        symb_wid = float(vocab_inv[input_ids[1].item()][2:-2])\n",
    "        symb_seq = [input_id.item() - vocab['0'] for input_id in input_ids[2:]]\n",
    "        return symb_type, symb_wid, symb_seq\n",
    "\n",
    "    def batch_decode(self, batch: torch.LongTensor) -> Tuple[torch.Tensor, torch.Tensor, list]:\n",
    "        batch = batch.cpu().numpy()\n",
    "        symb_types = []\n",
    "        symb_wids = []\n",
    "        symb_seqs = []\n",
    "        for input_ids in batch:\n",
    "            symb_type, symb_wid, symb_seq = self.decode(input_ids)\n",
    "            symb_types.append(symb_type)\n",
    "            symb_wids.append(symb_wid)\n",
    "            symb_seqs.append(symb_seq)\n",
    "        return torch.tensor(symb_types, dtype=torch.long), torch.tensor(symb_wids, dtype=torch.float), symb_seqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 5, 26, 37, 46, 39, 44,  0])\n",
      "<|BPSK|><|0.50|>0927<|eos|>\n",
      "(1, 0.5, [0, 9, 2, 7, -37])\n"
     ]
    }
   ],
   "source": [
    "tokenizer = SignalTokenizer(vocab)\n",
    "tok=tokenizer.encode(1, 0.5, np.array([0,9,2,7]))\n",
    "print(tok)\n",
    "print(tokenizer.decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SignalDataset(Dataset):\n",
    "    def __init__(self, data_path):\n",
    "        super(SignalDataset, self).__init__()\n",
    "        # Recursively find all csv files in the data_path\n",
    "        self.file_list = glob.glob(os.path.join(data_path, '**/*.csv'), recursive=True)\n",
    "        self.cache = {}  # Dictionary for caching data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.file_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        if index in self.cache:\n",
    "            return self.cache[index]\n",
    "        \n",
    "        data = pd.read_csv(self.file_list[index], header=None, names=['I', 'Q', 'Code Sequence', 'Modulation Type', 'Symbol Width'])\n",
    "        \n",
    "        iq_wave = data[['I', 'Q']].values\n",
    "        symb_seq = data['Code Sequence'].dropna().astype(int).values\n",
    "        symb_type = data['Modulation Type'].values[0]\n",
    "        symb_wid = data['Symbol Width'].values[0]\n",
    "\n",
    "        iq_wave = torch.tensor(iq_wave, dtype=torch.float32)\n",
    "        iq_wave = rearrange(iq_wave, 't c -> c t')\n",
    "        # Pad the features to 2048\n",
    "        iq_wave = torch.nn.functional.pad(iq_wave, (0, 2048 - iq_wave.shape[1]), mode='constant', value=0)\n",
    "\n",
    "        target = tokenizer.encode(symb_type, symb_wid, symb_seq)\n",
    "        # Cache processed data\n",
    "        self.cache[index] = (iq_wave, target)\n",
    "\n",
    "        return iq_wave, target\n",
    "    \n",
    "def _collator_fn(batch):\n",
    "    input_features = rnn_utils([item[0] for item in batch], batch_first=True)\n",
    "    labels = rnn_utils([item[1] for item in batch], batch_first=True, padding_value=-100)\n",
    "    return {\n",
    "        \"input_features\": input_features,\n",
    "        \"labels\": labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LogitsProcessor, LogitsProcessorList\n",
    "\n",
    "class SignalLogitsProcessor(LogitsProcessor):\n",
    "    def __init__(self):\n",
    "        super(SignalLogitsProcessor, self).__init__()\n",
    "        self.symb_type_mask = torch.tensor([vocab[symb_type_text] for symb_type_text in symb_type_dict.values()], dtype=torch.long)\n",
    "        self.symb_wid_mask = torch.tensor([vocab[f\"<|{symb_wid:.2f}|>\"] for symb_wid in torch.linspace(0,1,21)], dtype=torch.long)\n",
    "        self.symb_seq_mask = torch.tensor([i + vocab['0'] for i in range(32)], dtype=torch.long)\n",
    "\n",
    "    def __call__(self, input_ids, scores):\n",
    "        # The 1st token is the modulation type, the 2nd token is the symbol width, and the rest are the symbol sequence\n",
    "        new_scores = torch.full_like(scores, -float('inf'))\n",
    "        if input_ids.numel() == 1:\n",
    "            new_scores[:, self.symb_type_mask] = scores[:, self.symb_type_mask]\n",
    "        elif input_ids.numel() == 2:\n",
    "            new_scores[:, self.symb_wid_mask] = scores[:, self.symb_wid_mask]\n",
    "        else:\n",
    "            new_scores[:, self.symb_seq_mask] = scores[:, self.symb_seq_mask]\n",
    "        return new_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|bos|><|8QAM|><|0.15|>HHHHHHHHHHHHHHHHHH\n",
      "(tensor([5]), tensor([0.1500]), [[24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24, 24]])\n"
     ]
    }
   ],
   "source": [
    "logits_processor_list = LogitsProcessorList([SignalLogitsProcessor()])\n",
    "tok=model.generate(input_features=torch.randn(1, 2, 2048), logits_processor=logits_processor_list)\n",
    "print(tokenizer.batch_decode(tok))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-0.1966, -0.2493,  0.1239,  ...,  0.0000,  0.0000,  0.0000],\n",
       "         [ 0.0755,  0.1529,  0.1540,  ...,  0.0000,  0.0000,  0.0000]]),\n",
       " tensor([12, 23, 44, 43, 37, 38, 42, 41, 43, 38, 41, 37, 37, 43, 41, 37, 38, 37,\n",
       "         44, 40, 40, 40, 37, 38, 43, 41, 43, 37, 42, 40, 42, 40, 40, 40, 41, 43,\n",
       "         41, 39, 43, 38, 39, 44, 40, 38, 42, 41, 38, 43, 37, 43, 44, 43, 44, 38,\n",
       "         43, 37, 39, 40, 40, 37, 42, 41, 40, 44, 41, 41, 37, 38, 42, 38, 40, 40,\n",
       "         44, 42, 37, 40, 44, 43, 38, 41, 38, 44, 38, 37, 44, 40, 40, 43, 40, 41,\n",
       "         40, 38, 38, 37, 37, 43, 43, 41, 42, 38, 39, 39, 44, 41, 40, 43, 43, 40,\n",
       "         40, 39, 43, 43, 40, 38, 44, 40,  0]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the dataset into training and validation sets\n",
    "from torch.utils.data import random_split\n",
    "dataset = SignalDataset('train_data')\n",
    "train_size = int(0.99 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "train_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "def compute_metrics(pred):\n",
    "    pred_ids = pred.predictions\n",
    "    label_ids = pred.label_ids\n",
    "    batch_size = pred_ids.shape[0]\n",
    "    pred_symb_type, pred_symb_wid, pred_symb_seq = tokenizer.batch_decode(pred_ids)\n",
    "    label_symb_type, label_symb_wid, label_symb_seq = tokenizer.batch_decode(label_ids)\n",
    "\n",
    "    mt_score = (pred_symb_type == label_symb_type).float().mean() * 100\n",
    "\n",
    "    er = torch.abs((pred_symb_wid - label_symb_wid) / label_symb_wid)\n",
    "    sw_score = torch.clamp(100 - (er - 0.05) / 0.15 * 100, 0, 100).mean()\n",
    "\n",
    "    cq_score = 0\n",
    "    for i in range(batch_size):\n",
    "        symb_seq_hat = torch.tensor(pred_symb_seq[i], dtype=torch.long)\n",
    "        symb_seq_ground_truth = label_symb_seq[i]\n",
    "        symb_seq_ground_truth_len = symb_seq_ground_truth.numel()\n",
    "        symb_seq_hat_len = symb_seq_hat.numel()\n",
    "\n",
    "        if symb_seq_ground_truth_len == 0 or symb_seq_hat_len == 0:\n",
    "            continue\n",
    "        symb_seq_hat = F.pad(symb_seq_hat, (0, symb_seq_ground_truth_len - symb_seq_hat_len), \"constant\", 0)\n",
    "\n",
    "        cs = torch.cosine_similarity(\n",
    "            symb_seq_hat.float(), symb_seq_ground_truth.float().unsqueeze(0)\n",
    "        )\n",
    "        cq_score += torch.clamp((cs - 0.7) / 0.25 * 100, 0, 100) / batch_size\n",
    "\n",
    "    score = 0.2 * mt_score + 0.3 * sw_score + 0.5 * cq_score\n",
    "    return {\n",
    "        \"score\": score,\n",
    "        \"mt_score\": mt_score,\n",
    "        \"sw_score\": sw_score,\n",
    "        \"cq_score\": cq_score,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/222760 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "  0%|          | 1/222760 [00:17<1073:29:09, 17.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 4.4076, 'grad_norm': 16.228212356567383, 'learning_rate': 9.999955108637098e-05, 'epoch': 0.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    }
   ],
   "source": [
    "# Finetune Whisper on dataset\n",
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments\n",
    "\n",
    "training_args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./logs/whisper_iq\",\n",
    "    run_name=\"whisper_finetune\",\n",
    "    learning_rate=1e-4,\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=1,\n",
    "    logging_dir=\"./logs/whisper_iq\",\n",
    "    logging_steps=1,\n",
    "    metric_for_best_model=\"score\",\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=1000,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    data_collator=_collator_fn,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<|startoftranscript|>00000000000000000000']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_ids = model.generate(dataset[0][0].unsqueeze(0))\n",
    "tokenizer.batch_decode(output_ids)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
